https://github.com/huggingface/transformers/tree/master/examples/language-modeling

git clone https://github.com/huggingface/transformers.git
cd transformers/examples/language-modeling/
pip3 install -r requirements.txt
export TF_CPP_MIN_LOG_LEVEL=3
export TRANSFORMERS_VERBOSITY=error

GPT2
====

python3 run_clm.py \
    --model_name_or_path gpt2 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --do_train \
    --do_eval \
    --output_dir /content/tmp/test-clm \
    --fp16 \
    --per_device_train_batch_size 3 \
    --per_device_eval_batch_size 3

python3 run_clm.py --help

Roberta
=======

python3 run_mlm.py \
    --model_name_or_path roberta-base \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --do_train \
    --do_eval \
    --fp16 \
    --disable_tqdm False \
    --output_dir /content/tmp/test-mlm


# esperberto with run_language_modeling.py
==========================================

# https://huggingface.co/blog/how-to-train
# https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py
# https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/
in poetry2021
python3 train_esperberto_tokenizer.py
./train_esperberto2.py


# smallberta esperanto with run_mlm.py
======================================

cd esperberto/esperberto3_mlm
./train_esperberto3.sh

# smallberta with run_mlm.py
============================

## bpe tokenizer
cd src/pan_tadeusz2_mlm_bpetok
python3 train_tokenizer.py
./train_lm.sh

## stemmer and matching tokenizer

cd src/experiments/pan_tadeusz3_mlm_stem
./train_lm.sh

cd src/experiments/pan_tadeusz4_mlm_stem
./train_lm.sh

cd src/experiments/pan_tadeusz6_gpt2_lm
./install.sh
./train_lm.sh
