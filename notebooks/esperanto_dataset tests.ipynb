{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "critical-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\n",
    "class EsperantoDataset(Dataset):\n",
    "    max_len = 128\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tokenizer():\n",
    "        # /workspace/poetry2021.gt/data\n",
    "        data_path = Path('/workspace/poetry2021.gt/data/esperberto2')\n",
    "        tokenizer_path = data_path / 'tokenizer'\n",
    "\n",
    "        # tokenizer = ByteLevelBPETokenizer(\n",
    "        #     str(tokenizer_path / \"vocab.json\"),\n",
    "        #     str(tokenizer_path / \"merges.txt\"),\n",
    "        # )\n",
    "        # tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "        #     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        #     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        # )\n",
    "        # tokenizer.enable_truncation(max_length=512)\n",
    "        # tokenizer.mask_token = '<mask>'\n",
    "        # tokenizer._pad_token = '<pad>'\n",
    "        # tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        # tokenizer.padding_side = 'left'\n",
    "        # tokenizer.get_special_tokens_mask = None\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path, max_len=__class__.max_len)\n",
    "        return tokenizer\n",
    "\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        data_path = Path('/workspace/poetry2021.gt/data/esperberto2')\n",
    "        dataset_path = data_path / 'dataset'\n",
    "        tokenizer = __class__.get_tokenizer()\n",
    "        # tokenizer_path = data_path / 'tokenizer2'\n",
    "\n",
    "        # tokenizer = ByteLevelBPETokenizer(\n",
    "        #     tokenizer_path / \"vocab.json\",\n",
    "        #     tokenizer_path / \"merges.txt\",\n",
    "        # )\n",
    "        # tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "        #     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        #     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        # )\n",
    "        # tokenizer.enable_truncation(max_length=512)\n",
    "        # # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        # src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"*-train.txt\")\n",
    "        src_files = [dataset_path / 'oscar.eo.1000.txt']\n",
    "\n",
    "        for src_file in src_files:\n",
    "            print(\"ðŸ”¥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            lines = [x[:__class__.max_len] for x in lines]\n",
    "            # self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "            self.examples += [x for x in tokenizer.batch_encode_plus(lines).input_ids]\n",
    "\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "flush-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EsperantoDataset.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finite-hebrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1047, 2], [0, 446, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(['ala', 'ma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "specified-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ /workspace/poetry2021.gt/data/esperberto2/dataset/oscar.eo.1000.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = EsperantoDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-collector",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
