{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Njlw_Q14cYg1",
    "outputId": "895b39b8-c230-4445-a646-0ee831c5cb36",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  7 09:03:16 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 50%   63C    P0   175W / 350W |      1MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    On   | 00000000:67:00.0 Off |                  N/A |\n",
      "| 30%   57C    P0   159W / 350W |      1MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t5i7m_9ANDpF"
   },
   "outputs": [],
   "source": [
    "# !bash <(curl -s https://raw.githubusercontent.com/wojtekcz/language2motion/master/notebooks/Colab/swift_colab_ssh_server_bekaes.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Eq1yXbwBPGOO"
   },
   "outputs": [],
   "source": [
    "# !ps ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9QuDJaR_wdND"
   },
   "outputs": [],
   "source": [
    "# privateKeyPath=\"/root/.ssh/private_key.pem\"\n",
    "# options=\"-oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null\"\n",
    "# !ssh $options -i $privateKeyPath -f -R 8888:localhost:22 wcz@bekaes.beanflows.com -N -v &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1rev8D4XPPEb"
   },
   "outputs": [],
   "source": [
    "# !/etc/init.d/ssh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CVkKF5Q_PIA8"
   },
   "outputs": [],
   "source": [
    "# !kill -9 1602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# How to train a new language model from scratch using Transformers and Tokenizers\n",
    "\n",
    "### Notebook edition (link to blogpost [link](https://huggingface.co/blog/how-to-train)). Last update May 15, 2020\n",
    "\n",
    "\n",
    "Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n",
    "\n",
    "In this post we’ll demo how to train a “small” model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT. We’ll then fine-tune the model on a downstream task of part-of-speech tagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK7PPVm2XBgr"
   },
   "source": [
    "## 1. Find a dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g4-oH93eT8RV"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pqFEUzUTQN2",
    "outputId": "18af060e-5ab0-4f0f-8466-54192c516b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27988\n",
      "drwxr-xr-x 4 root root      147 Jan  7 09:02 .\n",
      "drwxr-xr-x 3 root root       27 Jan  7 09:02 ..\n",
      "-rw-r--r-- 1 root root   257251 Jan  7 09:02 pan_tadeusz.caps1.txt\n",
      "-rw-r--r-- 1 root root 27765959 Jan  7 09:02 pan_tadeusz.sampled1.txt\n",
      "-rw-r--r-- 1 root root   408976 Jan  7 09:02 pan_tadeusz.syl1.txt\n",
      "-rw-r--r-- 1 root root   223706 Dec 11 16:43 pan_tadeusz.txt\n",
      "drwxr-xr-x 2 root root       57 Jan  7 09:02 tmp\n",
      "drwxr-xr-x 2 root root       28 Jan  7 09:02 tokenizer1\n"
     ]
    }
   ],
   "source": [
    "dataset_path =   Path('data/rnn_generator'); dataset_path\n",
    "!mkdir -p $dataset_path\n",
    "!ls -la $dataset_path/\n",
    "tmp_path = dataset_path / 'tmp/'\n",
    "!mkdir -p $tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
     ]
    }
   ],
   "source": [
    "!apt -qq update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt -qq install -y wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGrNfvxCTLoX",
    "outputId": "62d494e9-cc19-4ad3-c7d9-50ccd943ae25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-07 09:03:21--  https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/pan_tadeusz.txt\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/529e1a80-3bd8-11eb-9ad6-7de957bbb808?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210107T090212Z&X-Amz-Expires=300&X-Amz-Signature=f44248386c4ade3948f6f160746b4721bfce0842365b83e484bc69f073792d4d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dpan_tadeusz.txt&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-01-07 09:03:22--  https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/529e1a80-3bd8-11eb-9ad6-7de957bbb808?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210107T090212Z&X-Amz-Expires=300&X-Amz-Signature=f44248386c4ade3948f6f160746b4721bfce0842365b83e484bc69f073792d4d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dpan_tadeusz.txt&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.38.180\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.38.180|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 223706 (218K) [application/octet-stream]\n",
      "Saving to: ‘data/rnn_generator/pan_tadeusz.txt.1’\n",
      "\n",
      "pan_tadeusz.txt.1   100%[===================>] 218.46K   507KB/s    in 0.4s    \n",
      "\n",
      "2021-01-07 09:03:23 (507 KB/s) - ‘data/rnn_generator/pan_tadeusz.txt.1’ saved [223706/223706]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P $dataset_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/pan_tadeusz.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LreedZYTfjyH",
    "outputId": "fef4aa7f-bc12-417f-d904-aa4bef589bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28M\n",
      "-rw-r--r-- 1 root root 252K Jan  7 09:02 pan_tadeusz.caps1.txt\n",
      "-rw-r--r-- 1 root root  27M Jan  7 09:02 pan_tadeusz.sampled1.txt\n",
      "-rw-r--r-- 1 root root 400K Jan  7 09:02 pan_tadeusz.syl1.txt\n",
      "-rw-r--r-- 1 root root 219K Dec 11 16:43 pan_tadeusz.txt\n",
      "-rw-r--r-- 1 root root 219K Dec 11 16:43 pan_tadeusz.txt.1\n",
      "drwxr-xr-x 2 root root   57 Jan  7 09:02 \u001b[0m\u001b[01;34mtmp\u001b[0m/\n",
      "drwxr-xr-x 2 root root   28 Jan  7 09:02 \u001b[01;34mtokenizer1\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls -l -h data/rnn_generator/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzZIXB-0zkmK"
   },
   "source": [
    "## Pobranie i instalacja stemmera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEmB620tjBKi",
    "outputId": "37436fc5-9558-411f-c0df-287eff48fb0e"
   },
   "outputs": [],
   "source": [
    "!dpkg --add-architecture i386\n",
    "!apt-get -qq update\n",
    "!apt-get -qq install libc6:i386 libncurses5:i386 libstdc++6:i386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMrR-5m1Tq15",
    "outputId": "ffb24059-f8ec-455e-a50c-41a137b58bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-07 09:03:28--  https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/stemmer-2.0.3.tgz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/b410ba00-3bd5-11eb-8cfd-a34708ba2e16?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210107T090225Z&X-Amz-Expires=300&X-Amz-Signature=97b5cd73bc934cbcec5092148866a89343e37c43d60783329a6dbf215fad1fb3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dstemmer-2.0.3.tgz&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-01-07 09:03:28--  https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/b410ba00-3bd5-11eb-8cfd-a34708ba2e16?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210107T090225Z&X-Amz-Expires=300&X-Amz-Signature=97b5cd73bc934cbcec5092148866a89343e37c43d60783329a6dbf215fad1fb3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dstemmer-2.0.3.tgz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.8.100\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.8.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2803075 (2.7M) [application/octet-stream]\n",
      "Saving to: ‘stemmer-2.0.3.tgz.1’\n",
      "\n",
      "stemmer-2.0.3.tgz.1 100%[===================>]   2.67M  2.13MB/s    in 1.3s    \n",
      "\n",
      "2021-01-07 09:03:30 (2.13 MB/s) - ‘stemmer-2.0.3.tgz.1’ saved [2803075/2803075]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/stemmer-2.0.3.tgz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KVvehDDWTwEW"
   },
   "outputs": [],
   "source": [
    "stemmer_path = Path('./') / 'stemmer-2.0.3.tgz'\n",
    "!tar xzf $stemmer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGEJUx4GkFUU",
    "outputId": "1cfcf3af-8e9b-4715-a107-6762f869318a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.2M\n",
      "drwxr-xr-x 2  501 staff  106 Apr 30  2018 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwx------ 1 root root  4.0K Jan  7 09:03 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1  501 staff 1.2K Apr 19  2018 changelog.txt\n",
      "-rwxr-xr-x 1  501 staff  629 Jan  2  2018 \u001b[01;32mdestem.sh\u001b[0m*\n",
      "-rwxrwxr-x 1  501 staff 3.7M Apr 19  2018 \u001b[01;32mstemmer.linux\u001b[0m*\n",
      "-rwxr-xr-x 1  501 staff 3.6M Apr 30  2018 \u001b[01;32mstemmer.macos\u001b[0m*\n",
      "-rw-r--r-- 1  501 staff 858K Nov 22  2017 stemmer2.dic\n"
     ]
    }
   ],
   "source": [
    "ls -lah bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsWp5K2ofjQz"
   },
   "source": [
    "## Instalacja bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WphVN9rmfiXs",
    "outputId": "a740fdbb-4ed3-4194-cc51-5bdc8a640379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
      "Requirement already satisfied: transformers==4.0.1 in /usr/local/lib/python3.6/dist-packages (4.0.1)\n",
      "Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (1.19.4)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (4.54.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (20.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (3.0.12)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.1.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.0.1) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2020.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.11.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.1) (1.0.0)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.0rc1\n",
      "    Uninstalling tokenizers-0.10.0rc1:\n",
      "      Successfully uninstalled tokenizers-0.10.0rc1\n",
      "Successfully installed tokenizers-0.9.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U sentencepiece transformers==4.0.1 nlp tokenizers==0.10.0rc1\n",
    "!pip install -U sentencepiece transformers==4.0.1 nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.10.0rc1\n",
      "  Using cached tokenizers-0.10.0rc1-cp36-cp36m-manylinux2010_x86_64.whl (3.2 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.0.1 requires tokenizers==0.9.4, but you have tokenizers 0.10.0rc1 which is incompatible.\u001b[0m\n",
      "Successfully installed tokenizers-0.10.0rc1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.10.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHlimami7b1"
   },
   "source": [
    "## Załadowanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WMhhgYDig_7G"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import platform\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "import psutil\n",
    "import pickle\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time, math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.ticker as ticker\n",
    "# %matplotlib inline\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# mpl.style.use('default')\n",
    "# mpl.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8KbaMvrXM5J"
   },
   "source": [
    "## Preprocessing korpusu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "80ZjgATAXsVn"
   },
   "outputs": [],
   "source": [
    "dataset_path = Path('data/rnn_generator'); dataset_path\n",
    "tmp_path = dataset_path / 'tmp/'\n",
    "!mkdir -p $tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPFXCZguyyXA",
    "outputId": "2faa5272-2123-4cca-c004-16cfc066933e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28M\n",
      "drwxr-xr-x 4 root root  172 Jan  7 09:03 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 3 root root   27 Jan  7 09:02 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root 252K Jan  7 09:02 pan_tadeusz.caps1.txt\n",
      "-rw-r--r-- 1 root root  27M Jan  7 09:02 pan_tadeusz.sampled1.txt\n",
      "-rw-r--r-- 1 root root 400K Jan  7 09:02 pan_tadeusz.syl1.txt\n",
      "-rw-r--r-- 1 root root 219K Dec 11 16:43 pan_tadeusz.txt\n",
      "-rw-r--r-- 1 root root 219K Dec 11 16:43 pan_tadeusz.txt.1\n",
      "drwxr-xr-x 2 root root   57 Jan  7 09:02 \u001b[01;34mtmp\u001b[0m/\n",
      "drwxr-xr-x 2 root root   28 Jan  7 09:02 \u001b[01;34mtokenizer1\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls -lah $dataset_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9eTRgNsMyyXD"
   },
   "outputs": [],
   "source": [
    "fn_corpus_char = dataset_path/'pan_tadeusz.txt'\n",
    "fn_corpus_caps = dataset_path/'pan_tadeusz.caps1.txt'\n",
    "fn_corpus_syl = dataset_path/'pan_tadeusz.syl1.txt'\n",
    "fn_corpus_sampled = dataset_path/'pan_tadeusz.sampled1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wuZu6mE057C"
   },
   "source": [
    "Plik wejściowy (korpus) to duży plik tekstowy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaV5Ly3R_tzj",
    "outputId": "fb7fb9a5-3935-4694-b96d-ecdf387ae0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿KSIĘGA PIÉRWSZA.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GOSPODARSTWO.\n",
      "\n",
      "\n",
      "TREŚĆ.\n",
      "\n",
      "    Powrot panicza -- Spotkanie się piérwsze w pokoiku, drugie u\n",
      "    stołu -- Ważna Sędziego nauka o grzeczności -- Podkomorzego uwagi\n",
      "    polityczne nad modami -- Początek sporu o Kusego i Sokoła -- Żale\n",
      "    Wojskiego -- Ostatni Woźny Trybunału -- Rzut oka na ówczesny stan\n",
      "    polityczny Litwy i Europy.\n",
      "\n",
      "\n",
      "  Litwo! Ojczyzno moja! ty jesteś jak zdrowie;\n",
      "Ile cię trzeba cenić, ten tylko się dowie\n",
      "Kto cię stracił. Dziś piękność twą w całéj ozdobie\n",
      "Widzę i opisuję, bo tęsknię po tobie.\n"
     ]
    }
   ],
   "source": [
    "!head -n 21 $fn_corpus_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKPynzs-yyW-"
   },
   "source": [
    "### Tokenizacja wielkich liter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enPpQXzj_oj6"
   },
   "source": [
    "Zamieniamy duże litery na małe dodając tokeny `_up_` (dla wyrazów pisanych wielkimi literami) lub `_cap_` (dla wyrazów pisanych z wielkiej litery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UJtiF_yZiA-z"
   },
   "outputs": [],
   "source": [
    "def do_caps(ss):\n",
    "  TOK_UP,TOK_CAP = ' _up_ ', ' _cap_ '\n",
    "  res = []\n",
    "  re_word = re.compile('\\w')\n",
    "  for s in re.findall(r'\\w+|\\W+', ss):\n",
    "      res += ([TOK_UP,s.lower()] if (s.isupper() and (len(s)>2))\n",
    "              else [TOK_CAP,s.lower()] if s.istitle()\n",
    "              else [s.lower()])\n",
    "  return ''.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cAVEtNiC9X",
    "outputId": "941ec71a-5d95-4c6b-8480-87fcb030ffb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241558"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tmp = fn_corpus_char.open('r').read()\n",
    "corpus_tmp = do_caps(corpus_tmp)\n",
    "# trim lines\n",
    "corpus_lines = [x.strip() for x in corpus_tmp.split('\\n')]\n",
    "corpus_tmp = '\\n'.join(corpus_lines)\n",
    "#\n",
    "fn_corpus_caps.open('w').write(corpus_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJlxNo8q-quJ",
    "outputId": "5468f613-cff1-49db-a5b2-6324cad572ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ _up_ księga  _up_ piérwsza.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_up_ gospodarstwo.\n",
      "\n",
      "\n",
      "_up_ treść.\n",
      "\n",
      "_cap_ powrot panicza --  _cap_ spotkanie się piérwsze w pokoiku, drugie u\n",
      "stołu --  _cap_ ważna  _cap_ sędziego nauka o grzeczności --  _cap_ podkomorzego uwagi\n",
      "polityczne nad modami --  _cap_ początek sporu o  _cap_ kusego i  _cap_ sokoła --  _cap_ żale\n",
      "_cap_ wojskiego --  _cap_ ostatni  _cap_ woźny  _cap_ trybunału --  _cap_ rzut oka na ówczesny stan\n",
      "polityczny  _cap_ litwy i  _cap_ europy.\n",
      "\n",
      "\n",
      "_cap_ litwo!  _cap_ ojczyzno moja! ty jesteś jak zdrowie;\n",
      "_cap_ ile cię trzeba cenić, ten tylko się dowie\n",
      "_cap_ kto cię stracił.  _cap_ dziś piękność twą w całéj ozdobie\n",
      "_cap_ widzę i opisuję, bo tęsknię po tobie.\n"
     ]
    }
   ],
   "source": [
    "!head -n 21 $fn_corpus_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbMBLa_nxp2b"
   },
   "source": [
    "### Podział korpusu na sylaby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OONrdDyb-mS6"
   },
   "source": [
    "Dzielimy korpus na sylaby programem `stemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "efaGn-TLyyXG"
   },
   "outputs": [],
   "source": [
    "platform_suffixes = {'Linux': 'linux', 'Darwin': 'macos'}\n",
    "platform_suffix = platform_suffixes[platform.system()]\n",
    "stemmer_bin = f'LD_PRELOAD=\"\" bin/stemmer.{platform_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "w3qYJ20EyyXI"
   },
   "outputs": [],
   "source": [
    "# !$stemmer_bin -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGbQYAUYyyXK",
    "outputId": "f2deef61-9c13-4cf0-ba1f-fc48c889373e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer 2.0.3 2018-04-19 (Linux i386)\n",
      "For Korrida database, spellchecker and hyphenator copyright (C) 1993-2018 Wojciech Czarnowski\n",
      "For Stemmer copyright (C) 2018 Krzysztof Wolk and Wojciech Czarnowski\n",
      "Wojciech Czarnowski: wojtek.czarnowski@gmail.com, +48(608)202-272\n",
      "Krzysztof Wolk: krz.wolk@gmail.com, +48(606)918-623\n",
      "\n",
      "Dictionary: \"bin/stemmer2.dic\"\n",
      "Input file: \"data/rnn_generator/pan_tadeusz.caps1.txt\"\n",
      "Output file: \"data/rnn_generator/pan_tadeusz.syl1.txt\"\n",
      "Stem number: \"7683\"\n",
      "\n",
      "Stemming options:\n",
      "  StemNiePrefix     : Yes\n",
      "  StemExtraPrefixes : Yes\n",
      "  StemPrefixesInRoot: No\n",
      "\n",
      "Syllable division options:\n",
      "  DivideWords          : Yes\n",
      "  DivideWithDictionary : Yes\n",
      "  DivideAlgorithmically: Yes\n",
      "  DivideUknkownWords   : Yes\n",
      "  divideAfterChar      : 1\n",
      "\n",
      "Stemming formatting options:\n",
      "  StemInSuffix       : No\n",
      "  ShowPOSInfo        : No\n",
      "  ShowExtraPOSInfo   : No\n",
      "\n",
      "  ShowGroupCode      : No\n",
      "  ShowBaseSuffixCodes: No\n",
      "  ShowSuffixCode     : No\n",
      "\n",
      "  stemDelimiterStr   : \"++ --\"\n",
      "  codeDelimiterStr   : \"@@\"\n",
      "\n",
      "StemFile(fileInPath: \"data/rnn_generator/pan_tadeusz.caps1.txt\", fileOutPath: \"data/rnn_generator/pan_tadeusz.syl1.txt\")\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!$stemmer_bin -s 7683 -v -d bin/stemmer2.dic -i $fn_corpus_caps -o $fn_corpus_syl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rurUv5wf_135",
    "outputId": "de2c8f20-b3d5-4209-e872-ed6965b38670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _up_ księ++ --ga  _up_ pié++ --rwsza.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_up_ go++ --s++ --po++ --dar++ --stwo.\n",
      "\n",
      "\n",
      "_up_ treść.\n",
      "\n",
      "_cap_ po++ --wrot pa++ --ni++ --cza --  _cap_ spot++ --ka++ --nie się pié++ --rwsze w po++ --koi++ --ku, dru++ --gie u\n",
      "sto++ --łu --  _cap_ waż++ --na  _cap_ sę++ --dzie++ --go na++ --u++ --ka o grze++ --cz++ --no++ --ści --  _cap_ pod++ --ko++ --mo++ --rze++ --go u++ --wa++ --gi\n",
      "po++ --li++ --ty++ --cz++ --ne nad mo++ --da++ --mi --  _cap_ po++ --czą++ --tek spo++ --ru o  _cap_ ku++ --se++ --go i  _cap_ so++ --ko++ --ła --  _cap_ ża++ --le\n",
      "_cap_ woj++ --skie++ --go --  _cap_ o++ --sta++ --t++ --ni  _cap_ woź++ --ny  _cap_ try++ --bu++ --na++ --łu --  _cap_ rzut oka na ów++ --czes++ --ny stan\n",
      "po++ --li++ --ty++ --cz++ --ny  _cap_ lit++ --wy i  _cap_ eu++ --ro++ --py.\n",
      "\n",
      "\n",
      "_cap_ lit++ --wo!  _cap_ oj++ --czyz++ --no mo++ --ja! ty je++ --s++ --teś jak zdro++ --wie;\n",
      "_cap_ ile cię trze++ --ba ce++ --nić, ten tyl++ --ko się do++ --wie\n",
      "_cap_ kto cię stra++ --cił.  _cap_ dziś pięk++ --ność twą w całéj o++ --zdo++ --bie\n",
      "_cap_ wi++ --dzę i o++ --pi++ --su++ --ję, bo tęs++ --knię po to++ --bie.\n"
     ]
    }
   ],
   "source": [
    "!head -n 21 $fn_corpus_syl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpf6QibJyyXL"
   },
   "source": [
    "### Załadowanie do pamięci i tokenizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSyGzgVD057R"
   },
   "source": [
    "Ładujemy korpus do pamięci i tokenizujemy. Tworzymy też listę wszystkich tokenów `all_tokens`. Mamy już specjalne tokeny `_cap_` i `_up_`, zamieniamy znaki końca lini na token `_eol_` i dodajemy token `_unk_` na wypadek, gdybyśmy użyli sylaby (tokena), który nie wystąpił wcześniej w korpusie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9H83p3sXM5L",
    "outputId": "80f4632d-50dc-45bd-86b8-3772661527d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 393286\n"
     ]
    }
   ],
   "source": [
    "file = open(fn_corpus_syl).read()\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maVrQMtZyyXU",
    "outputId": "adcf2aab-c17a-46f1-b620-4f907eef2973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6473 ['<unk>', '<pad>', '<mask>', '<s>', '</s>', '_eol_', '_cap_', '_up_', '!', '\"', '%', \"'\", '(', ')', ',', '--', '--a++', '--aczéj', '--aczéj:', '--ał,', '--b++', '--ba', '--ba++', '--ba,', '--ba.', '--ba;', '--bach', '--bach.', '--bach;', '--baj++', '--bak', '--bak,', '--ban', '--ban++', '--bar++', '--barz,', '--baw', '--baw++', '--bał', '--baż', '--bcem', '--bcem,', '--be++', '--bek', '--bel', '--bel++', '--belg', '--bem', '--bem,', '--ber++']\n"
     ]
    }
   ],
   "source": [
    "# taken from fastai/text.py\n",
    "\n",
    "# remove +,- chars from punctuation set to keep syllables e.g.'--PO++' intact\n",
    "# remove _ char to keep tokens intact\n",
    "# remove <,> chars to keep tokens intact\n",
    "punctuation=re.sub('[_\\+-<>]', '', string.punctuation)\n",
    "re_tok = re.compile(f'([{punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s, repl_unk=True): \n",
    "  strings = re_tok.sub(r' \\1 ', s).replace('\\n', ' _eol_ ').split()\n",
    "  if repl_unk:\n",
    "    strings = [str2tok(s) for s in strings]\n",
    "  return strings\n",
    "\n",
    "file_tok = tokenize(file, repl_unk=False); len(file_tok), file_tok[:8]\n",
    "file_tok_len = len(file_tok)\n",
    "\n",
    "spec_tokens = ['<unk>', '<pad>', '<mask>', '<s>', '</s>', '_eol_', '_cap_', '_up_']\n",
    "\n",
    "all_tokens = []\n",
    "all_tokens.extend(spec_tokens)\n",
    "all_tokens.extend(sorted(list(set([x for x in file_tok if not x in spec_tokens]))))\n",
    "n_tokens = len(all_tokens); print(n_tokens, all_tokens[:50])\n",
    "\n",
    "tok2idx_dict = {tok: idx for (idx, tok) in enumerate(all_tokens)}\n",
    "\n",
    "def str2tok(str) -> int:\n",
    "  return str if tok2idx_dict.get(str, 0) else all_tokens[0]\n",
    "\n",
    "def tok2idx(tok) -> int:\n",
    "  return tok2idx_dict.get(tok, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaNxdcoMADVS"
   },
   "source": [
    "Przyda nam się funkcja do zakodowania dowolnego tekstu na listę zsylabizowanych tokenów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rDQqpRYzxT4L"
   },
   "outputs": [],
   "source": [
    "def str2syl2tok(text):  \n",
    "  fn_tmp_text_caps = Path(tmp_path / 'tmp_text_caps1.txt')\n",
    "  fn_tmp_text_syl = Path(tmp_path / 'tmp_text_syl1.txt')\n",
    "  \n",
    "  text = do_caps(text)\n",
    "  fn_tmp_text_caps.open('w').write(text)\n",
    "  \n",
    "  !$stemmer_bin -s 7683 -d bin/stemmer2.dic -i $fn_tmp_text_caps -o $fn_tmp_text_syl\n",
    "  \n",
    "  text_syl = fn_tmp_text_syl.open('r').read()\n",
    "  \n",
    "  # kill last \\n eol char possibly added by stemmer\n",
    "  if text_syl[-1] == '\\n':\n",
    "    text_syl = text_syl[:-1]\n",
    "\n",
    "  text_tok = tokenize(text_syl, repl_unk=True)\n",
    "    \n",
    "  return text_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwnDdj75paVP",
    "outputId": "982bc3cf-3e45-4954-b413-bfd72868b661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_up_', 'lit++', '--wo', '!', '_cap_', 'oj++', '--czyz++', '--no', 'mo++', '--ja', '!', '_eol_', '_cap_', 'ty', 'je++', '--s++', '--teś', 'jak', 'zdro++', '--wie.', '_eol_', '_cap_', 'ile', 'cię', 'trze++', '--ba', 'ce++', '--nić', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "tekst = 'LITWO! Ojczyzno moja!\\nTy jesteś jak zdrowie.\\nIle cię trzeba cenić ble ble '\n",
    "tekst_tok = str2syl2tok(tekst); print(tekst_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAUKI80V6CvK"
   },
   "source": [
    "Funkcje pomocnicze do zdekodowania listy tokenów na tekst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzAAi_95yyXq",
    "outputId": "a51d53c3-d606-4345-fac4-3ef94ad051d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_up_ lit/wo ! _cap_ oj/czyz/no mo/ja ! _eol_ _cap_ ty je/s/teś jak zdro/wie. _eol_ _cap_ ile cię trze/ba ce/nić <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "def syl2str(a_list, delim='/'): \n",
    "  s = ' '.join(a_list)\n",
    "  \n",
    "  repl_list = [\n",
    "      ('++ --', delim), \n",
    "  ]\n",
    "  for repl in repl_list:\n",
    "    s = s.replace(repl[0], repl[1])\n",
    "  \n",
    "  return s\n",
    "\n",
    "print(syl2str(tekst_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUs-bnu5q2FF",
    "outputId": "ca3fdc71-8476-4ee1-ede8-e34b0783a7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LITWO ! Ojczyzno moja ! \n",
      " Ty jesteś jak zdrowie. \n",
      " Ile cię trzeba cenić <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "def decode_tokens(e_str):\n",
    "  # decode _eol_, _cap_ and _up_\n",
    "  # leave <unk> token alone\n",
    "  # kill <s> and </s>\n",
    "  e_syl = e_str.split(' ')\n",
    "  e_syl2 = []\n",
    "\n",
    "  cap = False; up = False\n",
    "\n",
    "  for syl in e_syl:\n",
    "    if syl == '_eol_': syl = '\\n'\n",
    "\n",
    "    if syl not in ['_cap_', '_up_', '<s>', '</s>']:\n",
    "      if cap == True: syl = syl.title(); cap = False\n",
    "      if up == True: syl = syl.upper(); up = False        \n",
    "      e_syl2.append(syl)\n",
    "\n",
    "    if syl == '_cap_': cap = True\n",
    "    if syl == '_up_': up = True\n",
    "\n",
    "  return ' '.join(e_syl2)\n",
    "\n",
    "print(decode_tokens(syl2str(tekst_tok, delim=''))[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqbQLawfrV-n",
    "outputId": "cda2c251-e4e9-44a5-acdb-ef846df65679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LITWO! Ojczyzno moja! \n",
      "Ty jesteś jak zdrowie. \n",
      "Ile cię trzeba cenić <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "def fix_punctuation(s): \n",
    "  repl_list = [\n",
    "      ('\\n ', '\\n'), \n",
    "      (' ,', ','),\n",
    "      (' .', '.'),\n",
    "      (' !', '!'),\n",
    "      (' ?', '?'),\n",
    "      (' ;', ';'),\n",
    "      ('( ', '('),\n",
    "      (' )', ')'),\n",
    "      (' «', '«'),\n",
    "      ('» ', '»'),\n",
    "      (' :', ':')\n",
    "  ]\n",
    "  \n",
    "  for repl in repl_list:\n",
    "    s = s.replace(repl[0], repl[1])\n",
    "  \n",
    "  return s\n",
    "\n",
    "print(fix_punctuation(decode_tokens(syl2str(tekst_tok, delim='')))[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2VJHqQmCyuT"
   },
   "source": [
    "Sformatujmy zdekodowany tekst w HTML i zaznaczmy na czerwono sylaby, z których nie dało się skleić słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "WFj5nGp5yyYI",
    "outputId": "e9c533f5-7fae-44f4-ad78-62dcbb721eb0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "LITWO! Ojczyzno moja! \n",
       "<br/>Ty jesteś jak zdrowie. \n",
       "<br/>Ile cię trzeba cenić <unk> <unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class X(str):\n",
    "    def rpl(self, p, c='lightgray'):\n",
    "        return X(self.replace(p, f'<font color=\"{c}\">{p}</font>'))\n",
    "    def rpl2(self, p, p2):\n",
    "        return X(self.replace(p, p2))\n",
    "      \n",
    "def format_html(e_str):\n",
    "  return X(e_str).rpl('/').rpl('--', c='red').rpl('++', c='red').rpl2('\\n', '\\n<br/>')\n",
    "\n",
    "e_str = fix_punctuation(decode_tokens(syl2str(tekst_tok, delim='')))[:400]\n",
    "e_html = format_html(e_str); display(HTML(e_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHYQxWpQUZHz",
    "outputId": "b5287e62-2da2-4f4d-ad42-ae6104bfb2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_GPU=True\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = torch.cuda.is_available(); \n",
    "# USE_GPU = False; \n",
    "\n",
    "print(f'USE_GPU={USE_GPU}')\n",
    "\n",
    "def to_gpu(x, *args, **kwargs):\n",
    "    return x.cuda(*args, **kwargs) if USE_GPU else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HC6CTIkdFiI6"
   },
   "outputs": [],
   "source": [
    "# Turn token list into list of longs\n",
    "def tok_tensor(token_list, pad_to = None):\n",
    "    tensor_len = len(token_list)\n",
    "    if pad_to:\n",
    "        tensor_len = pad_to\n",
    "\n",
    "    tensor = torch.zeros(tensor_len).long()\n",
    "    for c in range(len(token_list)):\n",
    "        tensor[c] = tok2idx(token_list[c])\n",
    "    \n",
    "    return tensor\n",
    "    # return Variable(to_gpu(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sjJFcYJDzZ2"
   },
   "source": [
    "Wskaźnik liczby sylab, z których nie dało się skleić słów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "tY2U1vt4yyYK"
   },
   "outputs": [],
   "source": [
    "# def bad_words(e_syl): e_str = syl2str(e_syl); return (e_str.count('++') + e_str.count('--')) / len(e_syl)\n",
    "def bad_words(e_str): e_syl = e_str.split(' '); return (e_str.count('++') + e_str.count('--')) / len(e_syl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azG7M3GxVnX1"
   },
   "source": [
    "## Roberta LM colator with stemmed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW4pvcsAZKLd"
   },
   "source": [
    "### Sample chunk_len token-sized chunks to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "r6PdNWoaZKI4"
   },
   "outputs": [],
   "source": [
    "# sample chunks into line by line dataset\n",
    "# sample and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrLRwR3rZKFo",
    "outputId": "a06b1531-ab98-4618-8497-2eef4e8f98e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734, 73444)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_len = 100 #400\n",
    "\n",
    "# def random_chunk():\n",
    "#     start_index = random.randint(0, file_tok_len - chunk_len -1)\n",
    "#     end_index = start_index + chunk_len + 1\n",
    "#     return file_tok[start_index:end_index]\n",
    "  \n",
    "n_samples = file_tok_len // chunk_len; n_samples, file_tok_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Cbv4Jh9Rtl4i"
   },
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "class LineChunker:\n",
    "    def __init__(self, file_tok: [str], chunk_len: int):\n",
    "        file_str = \" \".join(file_tok)\n",
    "        file_lines = [(x.strip() + ' _eol_').strip() for x in file_str.split('_eol_')]\n",
    "        self.file_lines_tok = [x.split() for x in file_lines]\n",
    "        self.chunk_len = chunk_len\n",
    "        self.last_num_lines = self.count_tok_lines(self.file_lines_tok[::-1], chunk_len=self.chunk_len)\n",
    "        self.last_line_index = len(self.file_lines_tok) - self.last_num_lines\n",
    "\n",
    "    @staticmethod\n",
    "    def count_tok_lines(file_lines_tok: [[str]], chunk_len: int):\n",
    "        # count how many last lines (almost) add up to chunk_len\n",
    "        sum_tok = 0\n",
    "        idx = 0\n",
    "        while True:\n",
    "            n = len(file_lines_tok[idx])\n",
    "            if sum_tok+n >= chunk_len or idx+1 >= len(file_lines_tok):\n",
    "                break\n",
    "            sum_tok += n\n",
    "            idx += 1\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def random_chunk(self):\n",
    "        start_index = random.randint(0, self.last_line_index)\n",
    "\n",
    "        # print(f'len(file_lines_tok): {len(self.file_lines_tok)}, last_num_lines: {self.last_num_lines}, last_line_index: {self.last_line_index}')\n",
    "\n",
    "        num_lines = self.count_tok_lines(self.file_lines_tok[start_index:], chunk_len=self.chunk_len)\n",
    "        end_index = start_index + num_lines\n",
    "\n",
    "        # print(f'start_index: {start_index}, end_index: {end_index}')\n",
    "        # print(f'num_lines: {num_lines}')\n",
    "        return flatten(self.file_lines_tok[start_index:end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "MAiYtiqHtsfu"
   },
   "outputs": [],
   "source": [
    "line_chunker = LineChunker(file_tok=file_tok, chunk_len=chunk_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnIPjix5Y6D9"
   },
   "source": [
    "Let's make dataset with more than minimum n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0Kc69ipYzzf",
    "outputId": "5678eab0-685c-4c94-ce95-3fc5b6595f72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = max(50000, n_samples); n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "qFH2loJTZKCl"
   },
   "outputs": [],
   "source": [
    "sampled_chunks = [\" \".join(line_chunker.random_chunk()) for _ in range(n_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "fUvXck1vZKAD",
    "outputId": "c1b517af-b780-4885-f388-1a5c63d3c747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_cap_ ciot++ --ka mnie za++ --my++ --ka++ --ła; nic by++ --ło s kim tań++ --czyć, _eol_ _cap_ lu++ --bi++ --łam z nu++ --dy pta++ --stwo paść i dzie++ --ci niań++ --czyć; _eol_ _cap_ ale po++ --cze++ --kaj cio++ --ciu, nie++ --chno się po++ --ba++ --wię _eol_ _cap_ tro++ --chę z lu++ --dźmi, o++ --ba++ --czysz jak się ja po++ --pra++ --wię. _eol_ _eol_ _cap_ już, rzek++ --ła ciot++ --ka, z dwoj++ --ga złe++ --go, le++ --piej s pta++ --stwem _eol_ _cap_ niż s tém co u nas do++ --tąd go++ --ści++ --ło plu++ --ga++ --stwem; _eol_'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y61mSFXKZJ9L",
    "outputId": "7e67df39-e2a5-4c98-b406-7a554006394c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26769217"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " fn_corpus_sampled.write_text(\"\\n\".join(sampled_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnVEd5DWZJ6o",
    "outputId": "a1a267c7-2a1c-4aec-ef85-d2d0a5098a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/rnn_generator/pan_tadeusz.sampled1.txt')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_corpus_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-kkz81OY6xH"
   },
   "source": [
    "## 2. Create a tokenizer\n",
    "\n",
    "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000.\n",
    "\n",
    "We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dgsy0oxhOn9"
   },
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Le7ZiHurd0EV"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace, CharDelimiterSplit\n",
    "from tokenizers.trainers import BpeTrainer #, WordLevelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "OaATNLmOuj7e"
   },
   "outputs": [],
   "source": [
    "# all_tokens[0:10]\n",
    "# fn_all_tokens = dataset_path/'all_tokens.txt'\n",
    "# fn_all_tokens.write_text(\" \".join(all_tokens))\n",
    "# len(all_tokens)\n",
    "# len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "x02uOI5TJUke"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel({tok:idx for idx, tok in enumerate(all_tokens)}))\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.pre_tokenizer = CharDelimiterSplit(' ')\n",
    "tokenizer.model.unk_token = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5OoJ6l-dz7-",
    "outputId": "f965349c-5a6c-44f6-99c8-e4564633a51f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6473"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "3MQLOG8sV7xb"
   },
   "outputs": [],
   "source": [
    "tokenizer_path = dataset_path / 'tokenizer1'\n",
    "tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save(str(tokenizer_path/\"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlJLVYp6ncQt"
   },
   "source": [
    "## Load roberta compatible tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "CfWLnnkRvhTl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "# https://github.com/huggingface/transformers/issues/7234#issuecomment-720092292\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NUqSWSxhLCo",
    "outputId": "ec1dfea7-d149-4c21-e4f5-7569987c434d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/rnn_generator/tokenizer1')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_path = dataset_path / 'tokenizer1'; tokenizer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Yc-Guq21vUss"
   },
   "outputs": [],
   "source": [
    "# Load it using transformers\n",
    "tokenizer2 = PreTrainedTokenizerFast(tokenizer_file=str(tokenizer_path/\"tokenizer.json\"))\n",
    "tokenizer2._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer2._tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer2._tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer2._tokenizer.enable_truncation(max_length=128) # 512\n",
    "tokenizer2.mask_token = \"<mask>\"\n",
    "tokenizer2.pad_token = \"<pad>\"\n",
    "# tokenizer2.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "iUC0J2_DjdgT"
   },
   "outputs": [],
   "source": [
    "# tokenizer2._tokenizer.token_to_id(\"</s>\")\n",
    "# tokenizer.token_to_id(\"<s>\")\n",
    "# tokenizer2(\"_cap_ lit++ --wo !\")\n",
    "# tokenizer.encode(\"_cap_ lit++ --wo !\")\n",
    "# tokenizer.encode(\"_cap_ lit++ --wo !\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZED6A_qCuJDL",
    "outputId": "d914a833-7887-4cb9-a230-3b1e6fc272ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 6, 4858, 3492, 8, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(\"_cap_ lit++ --wo !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Fddrm8ygDg8V"
   },
   "outputs": [],
   "source": [
    "# tokenizer2.save_vocabulary(str(tokenizer_path/\"tokenizer2.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "AHrropcIEuLP"
   },
   "outputs": [],
   "source": [
    "# tokenizer2._tokenizer.save(str(tokenizer_path/\"_tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "yZOkAL9SEuHa"
   },
   "outputs": [],
   "source": [
    "# tokenizer2._tokenizer.post_processor.num_special_tokens_to_add( #.save(str(tokenizer_path/\"post_processor.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB31vbsGEuEQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQpUC_CDhnWW"
   },
   "source": [
    "## 3. Train a language model from scratch\n",
    "\n",
    "**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n",
    "\n",
    "> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
    "\n",
    "As the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "kD140sFjh0LQ"
   },
   "outputs": [],
   "source": [
    "# Check that we have a GPU\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZZs-r6iKAV",
    "outputId": "76a45e4f-a2de-4012-ed55-463558a366a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0qQzgrBi1OX"
   },
   "source": [
    "### We'll define the following config for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMO3NhLMPtPa",
    "outputId": "c66653fb-152a-4380-afad-9b156aba0d75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6473, 6473, 6473)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens), tokenizer.get_vocab_size(), tokenizer2._tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "LTXXutqeDzPi"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer2._tokenizer.get_vocab_size(),\n",
    "    hidden_size=240,\n",
    "    intermediate_size=2048,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    "    bos_token_id=tokenizer2._tokenizer.token_to_id(\"<s>\"),\n",
    "    eos_token_id=tokenizer2._tokenizer.token_to_id(\"</s>\"),\n",
    "    pad_token_id=tokenizer2._tokenizer.token_to_id(\"<pad>\"),\n",
    "    # attention_probs_dropout_prob=0.0,\n",
    "    # hidden_dropout_prob=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afAqVhRly_bm",
    "outputId": "227f9bd0-b4a0-42fc-fe73-f8c6242a7bf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 3,\n",
       "  \"eos_token_id\": 4,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 240,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"vocab_size\": 6473\n",
       "}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAwQ82JiE5pi"
   },
   "source": [
    "Now let's re-create our tokenizer in transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yNCw-3hFv9h"
   },
   "source": [
    "Finally let's initialize our model.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "BzMqR-dzF4Ro"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "tYU4_jALiObb"
   },
   "outputs": [],
   "source": [
    "# !tar xzvf \"/content/PanTadeuszRoBERTa 6.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "3DS3YXa2hRMJ"
   },
   "outputs": [],
   "source": [
    "# model = RobertaForMaskedLM.from_pretrained(\"PanTadeuszRoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jU6JhBSTKiaM",
    "outputId": "36b75eb5-3a29-46d5-839c-9a2387f9d9b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9048281"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtUHRMliOLM"
   },
   "source": [
    "### Now let's build our training Dataset\n",
    "\n",
    "We'll build our dataset by applying our tokenizer to our text file.\n",
    "\n",
    "Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C66fQgiuSZH",
    "outputId": "df8af5de-1df4-4b3a-9e4b-a0c888f246e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/rnn_generator/pan_tadeusz.sampled1.txt')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_corpus_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlvP_A-THEEl",
    "outputId": "bac23998-94d5-4c29-cc3c-339c7dff60da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 s, sys: 594 ms, total: 28.5 s\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer2,\n",
    "    file_path=fn_corpus_sampled, #pan_tadeusz.txt\n",
    "    block_size=128, #512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDLs73HcIHk5"
   },
   "source": [
    "Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n",
    "\n",
    "This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "zTgWPa9Dipk2"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer2, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2BIQKqjfHm"
   },
   "source": [
    "### Finally, we are all set to initialize our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "YpvnFFmZJD-N"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"PanTadeuszRoBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=300,\n",
    "    per_device_train_batch_size=768, #64\n",
    "    logging_steps=100,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    # prediction_loss_only=True,\n",
    "    learning_rate=1e-3, #5e-05,\n",
    "    fp16=True,\n",
    "#     fp16_opt_level=\"O1\",\n",
    "#     fp16_backend=\"amp\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNRWUGFH1jij",
    "outputId": "b2252708-7b78-44b5-ab99-fa7caed22370"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='PanTadeuszRoBERTa', overwrite_output_dir=True, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=768, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=300, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan07_09-03-44_931ad7dc4059', logging_first_step=False, logging_steps=100, save_steps=2000, save_total_limit=1, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='PanTadeuszRoBERTa', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import apex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6sASa36Nf-N"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "id": "VmaHZXzmkNtJ",
    "outputId": "156f613b-b645-4aa0-82cf-fd17a17fab87",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='478' max='9900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 478/9900 04:56 < 1:37:47, 1.61 it/s, Epoch 14.45/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.610394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.977148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.343643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.290874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-068f9f09d0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_use_native_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_use_apex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m1cJZbbXd1g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZkooHz1-_2h"
   },
   "source": [
    "#### 🎉 Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDNgPls7_l13"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"PanTadeuszRoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AV1J1slEqNHL"
   },
   "outputs": [],
   "source": [
    "# killing checkpoints before tjgz-ting model\n",
    "check_path = (Path(\"PanTadeuszRoBERTa\")/'checkpoint-2000')#/'.ipynb_checkpoints'\n",
    "[x.unlink() for x in check_path.iterdir()]\n",
    "check_path.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7RWs2cs3fbn"
   },
   "outputs": [],
   "source": [
    "!tar czvf PanTadeuszRoBERTa.tgz PanTadeuszRoBERTa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0caceCy_p1-"
   },
   "source": [
    "## 4. Check that the LM actually trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIQJ8ND_AEhl"
   },
   "source": [
    "Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n",
    "\n",
    "Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n4O82t-2QhD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1N4xfXs081Z"
   },
   "source": [
    "## load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F3tG48f08wa"
   },
   "outputs": [],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP5UyC1_08q6"
   },
   "outputs": [],
   "source": [
    "!tar xzvf PanTadeuszRoBERTa.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0WaWCag1yZW"
   },
   "outputs": [],
   "source": [
    "model = model.from_pretrained(\"PanTadeuszRoBERTa\")\n",
    "model = to_gpu(model)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x01BrTs7cHW"
   },
   "source": [
    "## generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HctMe1UeiSlx"
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmruIsCMOy6y"
   },
   "outputs": [],
   "source": [
    "def bad_words(e_str): e_syl = e_str.split(' '); return (e_str.count('++') + e_str.count('--')) / len(e_syl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6WKwITMYTr4"
   },
   "outputs": [],
   "source": [
    "def print_eval(generated):\n",
    "    print(f'bad_words: {bad_words(generated)}')\n",
    "    e_syl = generated.split(' ')\n",
    "    decoded = decode_tokens(syl2str(e_syl, delim=''))\n",
    "    display(HTML(format_html(fix_punctuation(decoded))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9V9DYMmYqnO"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str, max_length=100, temperature=0.8):\n",
    "    prime_tok = str2syl2tok(prime_str)\n",
    "    # prime_tok\n",
    "    prime_tok_str = \" \".join(prime_tok)\n",
    "    # prime_tok_str\n",
    "    ids = tokenizer2.encode(prime_tok_str, return_tensors=\"pt\")[:,:-1]\n",
    "    preds = model.generate(ids.to(model.device), max_length=max_length, \n",
    "                           temperature=temperature, \n",
    "                           num_beams=10, early_stopping=True,\n",
    "                           no_repeat_ngram_size=2,\n",
    "                           do_sample=True,\n",
    "                           top_k=50,\n",
    "                           top_p=0.92\n",
    "                           )\n",
    "    return tokenizer2.decode(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7HelTpOWAc9"
   },
   "outputs": [],
   "source": [
    "gen1 = evaluate('chwycił na taśmie przypięty', max_length=300, temperature=1.0)\n",
    "print_eval(gen1)\n",
    "gen1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTCs0Vgamxqw"
   },
   "outputs": [],
   "source": [
    "print_eval(evaluate('Litwo! Ojczyzno', max_length=500, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqkV4XtAbnzU"
   },
   "outputs": [],
   "source": [
    "print_eval(evaluate('Litwo! Ojczyzno', max_length=500, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBkASwp-6gGi"
   },
   "outputs": [],
   "source": [
    "print_eval(evaluate('Litwo! Ojczyzno', max_length=500, temperature=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7sXhygC6hPB"
   },
   "outputs": [],
   "source": [
    "print_eval(evaluate('Tadeusz', max_length=500, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_v34TPkRpjhd"
   },
   "outputs": [],
   "source": [
    "print_eval(evaluate('Moskale', max_length=500, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cTjifqmpjdb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdVh0Xn7pjT9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Roberta_01_how_to_train, woj v6.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
