{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pan_tadeusz_gpt2_finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_6-SY9Dr_5y"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFOUGW-esktG"
      },
      "source": [
        "# upload private_key.pem and authorized_keys to /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldqikYFesWTC"
      },
      "source": [
        "!git clone https://github.com/wojtekcz/poetry2021.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslOn6uIsYyx"
      },
      "source": [
        "!poetry2021/scripts/setup_colab_finetuning.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XjmeCFAse6m"
      },
      "source": [
        "# !SSH_RELAY_HOST=wcz@bekaes.beanflows.com SSH_RELAY_PORT=8888 bash <(curl -s https://raw.githubusercontent.com/wojtekcz/poetry2021/master/colab_ssh/colab_ssh_server.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3qCwGiQ2OyZ"
      },
      "source": [
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNrqFOQT2TUI"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/poetry2021/src')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slhP-NIA2e-U"
      },
      "source": [
        "from preprocessing.text_tokenizer import TextTokenizer\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "904ad_5V2D6r"
      },
      "source": [
        "data_path = Path('/content/poetry2021/data/pan_tadeusz7')\n",
        "dataset_path = data_path / 'dataset'\n",
        "vocab_path = data_path / 'vocab.json'\n",
        "tokenizer_path = data_path / 'tokenizer'\n",
        "\n",
        "text_tokenizer = TextTokenizer(dataset_path)\n",
        "text_tokenizer.load_vocab(vocab_path)\n",
        "\n",
        "vocab = text_tokenizer.vocab\n",
        "vocab_count = len(vocab.keys())\n",
        "vocab.update({'<|endoftext|>': vocab_count})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qcOpID1qBh"
      },
      "source": [
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "print(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBd0uIT3HLO"
      },
      "source": [
        "models_path = Path('/content/poetry2021/data/pan_tadeusz7_lm_models')\n",
        "model_path = models_path / 'model1000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNerwYTB3XOh"
      },
      "source": [
        "USE_GPU = torch.cuda.is_available()\n",
        "# USE_GPU = False\n",
        "print(f'USE_GPU={USE_GPU}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "251qsRol3WwY"
      },
      "source": [
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTqcoQia1-z3"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
        "model = to_gpu(model)\n",
        "model.device\n",
        "\n",
        "# generate\n",
        "model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UuybHwc1-xD"
      },
      "source": [
        "def print_eval(generated):\n",
        "    # print(f'bad_words: {bad_words(generated)}')\n",
        "    e_syl = generated.split(' ')\n",
        "    decoded = text_tokenizer.decode_caps(text_tokenizer.syl2str(e_syl, delim=''))\n",
        "    print(text_tokenizer.fix_punctuation(decoded))\n",
        "    # display(HTML(text_tokenizer.format_html(text_tokenizer.fix_punctuation(decoded))))\n",
        "\n",
        "\n",
        "def evaluate(prime_str, max_length=100, temperature=0.8):\n",
        "    prime_tok = text_tokenizer.str2syl2tok(prime_str)\n",
        "    prime_tok_str = \" \".join(prime_tok)\n",
        "    ids = tokenizer.encode(prime_tok_str, return_tensors=\"pt\")[:, :-1]\n",
        "    preds = model.generate(ids.to(model.device), max_length=max_length,\n",
        "                           temperature=temperature,\n",
        "                           num_beams=10, early_stopping=True,\n",
        "                           no_repeat_ngram_size=2,\n",
        "                           do_sample=True,\n",
        "                           top_k=50,\n",
        "                           top_p=0.92\n",
        "                           )\n",
        "    return tokenizer.decode(preds[0])\n",
        "\n",
        "\n",
        "max_length = 500\n",
        "gen1 = evaluate('chwycił na taśmie przypięty', max_length=max_length, temperature=1.0)\n",
        "print_eval(gen1)\n",
        "gen1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKWudDhe1-uc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3JRNbZi1-qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Z5OtRU1-nC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9iLqZdw1-Y8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}