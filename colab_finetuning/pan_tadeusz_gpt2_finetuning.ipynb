{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pan_tadeusz_gpt2_finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_6-SY9Dr_5y"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFOUGW-esktG"
      },
      "source": [
        "# upload private_key.pem and authorized_keys to /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldqikYFesWTC"
      },
      "source": [
        "!git clone https://github.com/wojtekcz/poetry2021.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslOn6uIsYyx"
      },
      "source": [
        "!poetry2021/scripts/setup_colab_finetuning.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XjmeCFAse6m"
      },
      "source": [
        "# !SSH_RELAY_HOST=wcz@bekaes.beanflows.com SSH_RELAY_PORT=8888 bash <(curl -s https://raw.githubusercontent.com/wojtekcz/poetry2021/master/colab_ssh/colab_ssh_server.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3qCwGiQ2OyZ"
      },
      "source": [
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNrqFOQT2TUI"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/poetry2021/src')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slhP-NIA2e-U"
      },
      "source": [
        "from preprocessing.text_tokenizer import TextTokenizer\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "904ad_5V2D6r"
      },
      "source": [
        "data_path = Path('/content/poetry2021/data/pan_tadeusz7')\n",
        "dataset_path = data_path / 'dataset'\n",
        "vocab_path = data_path / 'vocab.json'\n",
        "tokenizer_path = data_path / 'tokenizer'\n",
        "\n",
        "text_tokenizer = TextTokenizer(dataset_path)\n",
        "text_tokenizer.load_vocab(vocab_path)\n",
        "\n",
        "vocab = text_tokenizer.vocab\n",
        "vocab_count = len(vocab.keys())\n",
        "vocab.update({'<|endoftext|>': vocab_count})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qcOpID1qBh"
      },
      "source": [
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "print(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRg9lK2_PqZw"
      },
      "source": [
        "# preprocess dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ9aImkWPqW0"
      },
      "source": [
        "from pathlib import Path\n",
        "from preprocessing.stemmer import Stemmer\n",
        "from preprocessing.line_chunker import LineChunker, flatten\n",
        "from preprocessing.text_processor import TextProcessor\n",
        "from preprocessing.text_tokenizer import TextTokenizer\n",
        "from typing import List\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU0RUxteQiba"
      },
      "source": [
        "dataset_path = data_path / 'dataset'\n",
        "fn_corpus_char = dataset_path / 'pan_tadeusz.txt'\n",
        "vocab_path = data_path / 'vocab.json'\n",
        "stem_delim = '++ --'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1aINwnKQgi5"
      },
      "source": [
        "def print_head(file_path, n_lines=10):\n",
        "    print('\\n'.join(file_path.read_text().split('\\n')[:n_lines]))\n",
        "\n",
        "\n",
        "class DatasetPreprocessor:\n",
        "    def __init__(self, dataset_path: Path, tokenizer: TextTokenizer):\n",
        "        self.tokenizer = tokenizer #TextTokenizer(dataset_path)\n",
        "        self.processor = TextProcessor(dataset_path, self.tokenizer)\n",
        "\n",
        "    def tokenize_caps(self, fn_corpus_char: Path, fn_corpus_caps: Path, verbose: True):\n",
        "        if verbose:\n",
        "            print(f'\\nTokenizacja wielkich liter: {fn_corpus_caps.name}')\n",
        "        self.processor.do_caps_file(fn_corpus_char, fn_corpus_caps)\n",
        "        if verbose:\n",
        "            print_head(fn_corpus_caps)\n",
        "\n",
        "    def stem_corpus(self, fn_corpus_caps: Path, fn_corpus_syl: Path, stem_delim: str, verbose: True):\n",
        "        if verbose:\n",
        "            print(f'\\nPodział korpusu na sylaby \"{stem_delim}\"')\n",
        "        Stemmer.stem_file(fn_corpus_caps, fn_corpus_syl, stem_delim=stem_delim)\n",
        "        if verbose:\n",
        "            print_head(fn_corpus_syl)\n",
        "\n",
        "    # def load_and_create_vocab(self, fn_corpus_syl: Path, vocab_path: Path) -> List[str]:\n",
        "    #     # Załadowanie do pamięci i tokenizacja\n",
        "    #     if fn_corpus_syl.is_dir():\n",
        "    #         file_tok = flatten([self.processor.load_and_tokenize_file(x, repl_unk=False) for x in fn_corpus_syl.glob('*.txt')])\n",
        "    #     else:\n",
        "    #         file_tok = self.processor.load_and_tokenize_file(fn_corpus_syl, repl_unk=False)\n",
        "\n",
        "    #     # create & save vocab\n",
        "    #     self.tokenizer.create_vocab(file_tok)\n",
        "    #     self.tokenizer.save_vocab(vocab_path)\n",
        "    #     return file_tok\n",
        "\n",
        "    def create_sampled_file(self, file_tok: List[str], fn_corpus_sampled: Path, min_n_samples: int, max_n_samples=None, chunk_len=100):\n",
        "        print(f\"\\nLet's make dataset with more than minimum {min_n_samples} samples\")\n",
        "        line_chunker = LineChunker(file_tok=file_tok, chunk_len=chunk_len)\n",
        "        n_samples = len(file_tok) // chunk_len\n",
        "        print(f'n_samples: {n_samples}')\n",
        "        n_samples = max(min_n_samples, n_samples)\n",
        "        if max_n_samples is not None:\n",
        "            n_samples = min(max_n_samples, n_samples)\n",
        "        print(f'chunk_len: {chunk_len}')\n",
        "        print(f'n_samples: {n_samples}')\n",
        "\n",
        "        sampled_chunks = [\" \".join(line_chunker.random_chunk()) for _ in tqdm(range(n_samples))]\n",
        "        fn_corpus_sampled.write_text(\"\\n\".join(sampled_chunks))\n",
        "        print(fn_corpus_sampled)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMrXeYQAQN9_"
      },
      "source": [
        "caps_path = dataset_path / 'caps'\n",
        "syl_path = dataset_path / 'syl'\n",
        "sampled_path = dataset_path / 'sampled'\n",
        "\n",
        "caps_path.mkdir(parents=True, exist_ok=True)\n",
        "syl_path.mkdir(parents=True, exist_ok=True)\n",
        "sampled_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Files to preprocess:')\n",
        "paths = [fn_corpus_char]\n",
        "# paths = [x for x in dataset_path.glob(\"**/*.txt\")]\n",
        "pprint(paths)\n",
        "\n",
        "preprocessor = DatasetPreprocessor(dataset_path, tokenizer=text_tokenizer)\n",
        "\n",
        "for char_path in paths:\n",
        "    print(f'tokenizing caps and stemming: {char_path.name}')\n",
        "    corpus_caps_path = caps_path / f'{char_path.stem}.caps1.txt'\n",
        "    corpus_syl_path = syl_path / f'{char_path.stem}.syl1.txt'\n",
        "\n",
        "    preprocessor.tokenize_caps(char_path, corpus_caps_path, verbose=False)\n",
        "    preprocessor.stem_corpus(corpus_caps_path, corpus_syl_path, stem_delim=stem_delim, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGYFMyP7Q-nA"
      },
      "source": [
        "text = 'LITWO! Ojczyzno moja!\\nTy jesteś jak zdrowie.\\nIle cię trzeba cenić ble ble '\n",
        "print(f'\\nTesting tokenizer: {text}')\n",
        "text_tok = text_tokenizer.str2syl2tok(text, stem_delim=stem_delim)\n",
        "print(text_tok)\n",
        "\n",
        "print(text_tokenizer.syl2str(text_tok, stem_delim=stem_delim))\n",
        "text_decoded = text_tokenizer.decode_caps(text_tokenizer.syl2str(text_tok, delim='', stem_delim=stem_delim))[:300]\n",
        "print(text_decoded)\n",
        "e_str = text_tokenizer.fix_punctuation(text_decoded)[:400]\n",
        "print(e_str)\n",
        "print(text_tokenizer.format_html(e_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYtCHhdaTSNp"
      },
      "source": [
        "# Sample dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86cA0IozRmEF"
      },
      "source": [
        "fn_corpus_syl = syl_path / 'pan_tadeusz.syl1.txt'\n",
        "file_tok = preprocessor.processor.load_and_tokenize_file(fn_corpus_syl, repl_unk=True)\n",
        "\n",
        "min_n_samples = 10000  # 50000\n",
        "max_n_samples = None\n",
        "chunk_len = 100  # 400\n",
        "fn_corpus_sampled = sampled_path / f'dataset.sampled1.{max_n_samples}.txt'\n",
        "preprocessor.create_sampled_file(file_tok, fn_corpus_sampled, min_n_samples=min_n_samples, max_n_samples=max_n_samples, chunk_len=chunk_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkR0H5igPqTr"
      },
      "source": [
        "# fine tune model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJduw80APqQh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSEVBbi5PqHr"
      },
      "source": [
        "# evaluate model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBd0uIT3HLO"
      },
      "source": [
        "models_path = Path('/content/poetry2021/data/pan_tadeusz7_lm_models')\n",
        "model_path = models_path / 'model1000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNerwYTB3XOh"
      },
      "source": [
        "USE_GPU = torch.cuda.is_available()\n",
        "# USE_GPU = False\n",
        "print(f'USE_GPU={USE_GPU}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "251qsRol3WwY"
      },
      "source": [
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTqcoQia1-z3"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
        "model = to_gpu(model)\n",
        "model.device\n",
        "\n",
        "# generate\n",
        "model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UuybHwc1-xD"
      },
      "source": [
        "def print_eval(generated):\n",
        "    # print(f'bad_words: {bad_words(generated)}')\n",
        "    e_syl = generated.split(' ')\n",
        "    decoded = text_tokenizer.decode_caps(text_tokenizer.syl2str(e_syl, delim=''))\n",
        "    print(text_tokenizer.fix_punctuation(decoded))\n",
        "    # display(HTML(text_tokenizer.format_html(text_tokenizer.fix_punctuation(decoded))))\n",
        "\n",
        "\n",
        "def evaluate(prime_str, max_length=100, temperature=0.8, num_beams=10, \n",
        "             early_stopping=True, no_repeat_ngram_size=2, do_sample=True, \n",
        "             top_k=50, top_p=0.92):\n",
        "    prime_tok = text_tokenizer.str2syl2tok(prime_str)\n",
        "    prime_tok_str = \" \".join(prime_tok)\n",
        "    ids = tokenizer.encode(prime_tok_str, return_tensors=\"pt\")[:, :-1]\n",
        "    preds = model.generate(ids.to(model.device), max_length=max_length,\n",
        "                           temperature=temperature,\n",
        "                           num_beams=num_beams, early_stopping=early_stopping,\n",
        "                           no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "                           do_sample=do_sample,\n",
        "                           top_k=top_k,\n",
        "                           top_p=top_p\n",
        "                           )\n",
        "    return tokenizer.decode(preds[0])\n",
        "\n",
        "\n",
        "max_length = 500\n",
        "# gen1 = evaluate('chwycił na taśmie przypięty', max_length=max_length, temperature=1.0)\n",
        "# print_eval(gen1)\n",
        "# gen1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKWudDhe1-uc"
      },
      "source": [
        "print_eval(evaluate('Litwo! Ojczyzno', max_length=max_length, temperature=1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3JRNbZi1-qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Z5OtRU1-nC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9iLqZdw1-Y8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}